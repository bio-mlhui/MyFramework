

1. 
现在的情况是, fusion后, video的特征输入multiscale encoder
如果video backbone输出后就用multiscale encoder, fusion后不使用

假设multiscale encoder是用来highlight objects,
那么第一种做法中, highlight是在video和text对齐之后; 并且是conditioned highlight
第二种做法中, highlight是在对齐之前, 没有任何条件加入,

对比两种做法, 第二种做法 对highlight的要求比较高, 因为要highlight出所有对象, 但是有利于对齐, 因为text对齐的对象是已经highlight好的object, 而不是没有结构的features

第一种做法中, 由于是 有条件的highlight, 所以highlight比较容易, 但是对齐的时候是没有结构的, 所以会造成highlight效果也不好


2. 
amr的self-attention, fusion, decoder中, attetion都没有加上1也没有使用</s><s>

3. 把decoder中的self attention 初始化为 amr2text 的self attention


4. 1e-4的学习率能够让模型收敛，但是后期有时候会出现震荡的情况
   1e-3的情况根本收敛不了
   怎么减小loss landscape的坑坑洼洼的情况?
    1. 增大batch可以减少训练的的随机性
    2. 

5. 没有matching的实验中的mask loss都是很小(0.), dice很大(3./0.), 两个的权重都是5
6. 如果有matching的话还可以让其他node的mask不是referent
7. 之前的实验中证明加上box后可以增大p@0.5, 
8. refereformer用的是*

9. 语言上的一些标注错误和其他
a. pos会有错误, 比如a light grey duch to the left of the light grey duck, 它会把light识别成第一个名词
   human being会识别成human是形容词,
b. amr parsing会有一些特点, 比如motor bike -> motocycle; being -> thing; human being -> human
   a cat is being licked by another cat, amr的tree上会有两个cat, c/c2, 但是有相同的alignment, 而且都是lick的arg0
b. 第一个名词, compound, nmod考虑在如何确立root上
    对于compound, 第一个名词可能和后面的某个单词构成compound, 并不一定是下一个, 出现了bug
d. a saddle and saddle blanket on a gray horse, 
e. 比如说有black,blue 这样的就会识别成一个单词

10. 
1. edge 和node有不同的维度
2. edge feature不变
3. 每层的graph layer共享
4. 注意用snt还是tokk, 因为假设linearized_amr.split() 是tokenizer的使用方式

11. 已经成了一个tree了

12. text 和amr一块用

13. 1.variable没有初始化为0 
    2. 不会有环的生成
    3. 
    4. 为什么tree-lstm不如v1

14. 
1. 加上object detector
2. 加上text sequence
3. 加上token classification损失
4. 使用多模态backbone
5. object有t个
6. 在object embedding上做文章比如concate坐标，object box difference
7, 舍弃variable, 全用concepts, 为每个predicate-specific argx都学一个embedding; 而且referent decoder中所有nodes, edges都做cross attention
8, 
15. 为什么先fusion再multiscale fusion更好
因为增加了复杂性, 先fusion再6层encoder 肯定比  6层encoder再fusion
16. 为什么p@0.9这么小?